{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:02:59.714543Z",
     "start_time": "2019-02-04T16:02:59.709887Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:02:59.969283Z",
     "start_time": "2019-02-04T16:02:59.956071Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:03:00.309544Z",
     "start_time": "2019-02-04T16:03:00.301949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Some of the columns we will look at can be quite wide, but it's good to get an idea of what they contain\n",
    "print(pd.get_option('max_colwidth'))\n",
    "pd.set_option('max_colwidth',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File/dir locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T14:49:55.813774Z",
     "start_time": "2019-02-04T14:49:55.809824Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "filename = \"preprocessed_taxon_pageseq_20190114_20190116.csv.gz\"\n",
    "# df_file = os.path.join(DATA_DIR, \"processed_journey\", filename)\n",
    "# df_reduced_file = os.path.join(DATA_DIR, \"processed_journey\", \"reduced_\"+filename)\n",
    "# df_rel_file = os.path.join(DATA_DIR, \"processed_journey\", \"rel_\"+filename)\n",
    "# df_doo_file = os.path.join(\n",
    "#     DATA_DIR, \"processed_journey\",\n",
    "#     \"doo_prelim_meta_standard_with_pageseq_from_29-10_to_04-11-2018.csv.gz\")\n",
    "\n",
    "df_dlo_file = os.path.join(\n",
    "    DATA_DIR, \"processed_journey\",\n",
    "    \"dlo_prelim_meta_standard_with_pageseq_from_29-10_to_04-11-2018.csv.gz\")\n",
    "df_kloo_file = os.path.join(\n",
    "    DATA_DIR, \"processed_journey\",\n",
    "    \"kloo_prelim_meta_standard_with_pageseq_from_29-10_to_04-11-2018.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T14:58:13.251383Z",
     "start_time": "2019-02-04T14:49:56.256851Z"
    }
   },
   "outputs": [],
   "source": [
    "#the 'drop length one' data read into pandas dataframe\n",
    "dlo = pd.read_csv(df_dlo_file, compression='gzip')\n",
    "#the 'keep length one only' data read into pandas dataframe\n",
    "kloo = pd.read_csv(df_kloo_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T14:58:13.428292Z",
     "start_time": "2019-02-04T14:58:13.339211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3788851, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T14:58:13.446260Z",
     "start_time": "2019-02-04T14:58:13.439735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890977, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kloo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up a data from two files: dlo = drop length one journeys, kloo = keep length one journeys only \n",
    "\n",
    "This data was produced by an early version of the pipeline and is missing some descriptive variables, such as taxons etc. However, it contains the sequences of pages and behaviours (or events) of users on those pages, including interaction with the sidebar and the related links contained therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T15:23:15.685611Z",
     "start_time": "2019-02-04T15:23:15.477848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6537680\n",
      "7650687\n"
     ]
    }
   ],
   "source": [
    "print(dlo['Occurrences'].sum())\n",
    "print(kloo['Occurrences'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:53:09.376433Z",
     "start_time": "2019-02-04T16:53:07.241082Z"
    }
   },
   "outputs": [],
   "source": [
    "#get a reproducible sample of 20% of journey types from each dataframe, \n",
    "#sampled in proportion to the number of occurrences of each journey type\n",
    "#then join the new samples together into a single dataframe\n",
    "\n",
    "# df = pd.concat([dlo.sample(frac=0.2, random_state=1234, weights=dlo.Occurrences).copy(), kloo.sample(frac=0.2, random_state=1234, weights=kloo.Occurrences).copy()], ignore_index=True)\n",
    "\n",
    "\n",
    "# try sampling with replacement, using occurrences as weights, but then \n",
    "# change all \"occurrences\" to 1, to try to create a more representative sample?\n",
    "# df = pd.concat([\n",
    "#     dlo.sample(\n",
    "#         frac=0.4, random_state=1234, weights=dlo.Occurrences, replace=True\n",
    "#     ).copy(),\n",
    "#     kloo.sample(\n",
    "#          frac=0.4, random_state=1234, weights=kloo.Occurrences, replace=True\n",
    "#     ).copy()],\n",
    "#     ignore_index=True)\n",
    "\n",
    "# try  concatting and THEN sampling with replacement, using occurrences as\n",
    "# weights, but then change all \"occurrences\" to 1, to try to create a more \n",
    "# representative sample?\n",
    "# df = pd.concat([\n",
    "#     dlo.copy(),\n",
    "#     kloo.copy()],\n",
    "#     ignore_index=True)\n",
    "# df = df.sample(\n",
    "#         frac=0.4, random_state=1234, weights=df.Occurrences, replace=True\n",
    "#     )\n",
    "\n",
    "# # try  concatting and THEN sampling without replacement, using occurrences as\n",
    "# # weights\n",
    "# df = pd.concat([\n",
    "#     dlo.copy(),\n",
    "#     kloo.copy()],\n",
    "#     ignore_index=True)\n",
    "# df = df.sample(\n",
    "#         frac=0.4, random_state=1234, weights=df.Occurrences\n",
    "#     )\n",
    "\n",
    "# try sampling with, using occurrences as weights, \n",
    "# and sum(Occurrences)*0.4 as n, but then change all \"occurrences\" to 1, \n",
    "# to try to create a more representative sample?\n",
    "# df = pd.concat([\n",
    "#     dlo.sample(\n",
    "#         n=math.ceil(0.4*dlo['Occurrences'].sum()), random_state=1234, \n",
    "#         weights=dlo.Occurrences, replace=True\n",
    "#     ).copy(),\n",
    "#     kloo.sample(\n",
    "#          n=math.ceil(0.4*kloo['Occurrences'].sum()), random_state=1234,\n",
    "#         weights=kloo.Occurrences, replace=True\n",
    "#     ).copy()],\n",
    "#     ignore_index=True)\n",
    "\n",
    "\n",
    "# try just concatting them\n",
    "df = pd.concat([\n",
    "    dlo[\n",
    "        ['DeviceCategories', 'Occurrences', 'Sequence', 'Event_cat_act_agg']\n",
    "    ].copy(),\n",
    "    kloo[\n",
    "        ['DeviceCategories', 'Occurrences', 'Sequence', 'Event_cat_act_agg']\n",
    "    ].copy()],\n",
    "    ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:53:09.386798Z",
     "start_time": "2019-02-04T16:53:09.380347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4679828, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove tablet occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:54:22.456736Z",
     "start_time": "2019-02-04T16:53:09.388872Z"
    }
   },
   "outputs": [],
   "source": [
    "def device_count(x, device):\n",
    "    return sum([value for item, value in x if item == device])\n",
    "df[\"TabletCount\"] = df['DeviceCategories'].apply(\n",
    "    ast.literal_eval).map(lambda x: device_count(x, \"tablet\"))\n",
    "df[\"Occurrences\"] = df[\"Occurrences\"] - df[\"TabletCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:54:23.745408Z",
     "start_time": "2019-02-04T16:54:22.459505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4294728, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Occurrences\"] != 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:52:44.670535Z",
     "start_time": "2019-02-04T16:52:44.617370Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAKE EACH OCCURRENCES 1\n",
    "# df['Occurrences'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## journey_click_rate\n",
    "There is no difference in the proportion of journeys using at least one related link (journey_click_rate) between page variant A and page variant B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of journeys including at least one click on a related link}}{\\text{total number of journeys}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total number of journeys including at least one click on a related link\n",
    "The numerator.\n",
    "\n",
    "We need to check within the Sequence column, whether the corresponding user journey has an Event where a related link was clicked. There is more than one level to this Event, we are specifically interested in \"Related content\" (as this is the sidebar of the page, the related links we are interested in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:54:48.924293Z",
     "start_time": "2019-02-04T16:54:48.917549Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Compute whether a journey includes at least one related link click\n",
    "def is_related(x):\n",
    "    return all(cond in x for cond in [\"relatedLinkClicked\",\"Related content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, `is_related` does not make sure that `relatedLinkClicked` and `Related content` exist in the same event in `Sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:55:03.066834Z",
     "start_time": "2019-02-04T16:54:49.296795Z"
    }
   },
   "outputs": [],
   "source": [
    "# map across the Sequence variable, which includes pages and Events\n",
    "# we want to pass all the list elements to a function one-by-one and then collect the output.\n",
    "df[\"Has_Related\"] = df[\"Sequence\"].map(is_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:55:03.188149Z",
     "start_time": "2019-02-04T16:55:03.069423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395771"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can filter for True and sum\n",
    "df[df[\"Has_Related\"]].Occurrences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total number of journeys\n",
    "The denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:55:03.222965Z",
     "start_time": "2019-02-04T16:55:03.191101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12971165"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Occurrences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this sample, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:55:03.340889Z",
     "start_time": "2019-02-04T16:55:03.233213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030511600153108838"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Has_Related\"]].Occurrences.sum() / df.Occurrences.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ratio of clicks on navigation elements vs. clicks on related links\n",
    "\n",
    "There is no statistically significant difference in the ratio of clicks on navigation elements vs. clicks on related links between page variant A and page variant B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\text{total number of navigation element click events from content pages}}{\\text{total number of related link click events}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total number of related link click events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to check `Related content` is in the event, because the `relatedLinkClicked` is also used for \"explore the topic\" links at the bottom of the page, with the event action containing `Explore the topic`, e.g. `(('relatedLinkClicked', '2.1 Explore the topic'), 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:30:35.229409Z",
     "start_time": "2019-02-04T16:30:35.204678Z"
    }
   },
   "outputs": [],
   "source": [
    "# If the event category is 'relatedLinkClicked' and the event action contains 'Related content', \n",
    "# return the count of that event\n",
    "def get_number_of_events_rl(event):\n",
    "    if event[0][0] == 'relatedLinkClicked' and 'Related content' in event[0][1]:\n",
    "        return event[1]\n",
    "    return 0\n",
    "\n",
    "def sum_related_click_events(event_list):\n",
    "    return sum([get_number_of_events_rl(event) for event in event_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:34:03.277220Z",
     "start_time": "2019-02-04T16:30:35.762510Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of related links clicks per Sequence\n",
    "df['Related Links Clicks per seq'] = df['Event_cat_act_agg'].apply(\n",
    "    ast.literal_eval).map(sum_related_click_events)\n",
    "\n",
    "# get the total number of related links clicks for that row (clicks per sequence multiplied by occurrences)\n",
    "df['Related Links Clicks row total'] = df['Related Links Clicks per seq'] * df['Occurrences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:34:03.326684Z",
     "start_time": "2019-02-04T16:34:03.282394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205595"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Related Links Clicks row total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "507px",
    "left": "62px",
    "top": "154px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
